apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {{ include "ollama.name" . }}
  labels:
    {{- include "ollama.labels" . | nindent 4 }}
spec:
  serviceName: {{ include "ollama.name" . }}
  replicas: {{ .Values.ollama.replicaCount }}
  selector:
    matchLabels:
      {{- include "ollama.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "ollama.selectorLabels" . | nindent 8 }}
    spec:
      containers:
      - name: {{ include "ollama.name" . }}
        image: {{ .Values.ollama.image }}
        ports:
        - containerPort: {{ .Values.ollama.servicePort }}
        env:
        {{- if .Values.ollama.gpu.enabled }}
          - name: PATH
            value: /usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
          - name: LD_LIBRARY_PATH
            value: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
          - name: NVIDIA_DRIVER_CAPABILITIES
            value: compute,utility
        {{- end}}
        {{- with .Values.ollama.resources }}
        resources: {{- toYaml . | nindent 10 }}
        {{- end }}
        volumeMounts:
        - name: ollama-volume
          mountPath: /root/.ollama
        tty: true
      {{- with .Values.ollama.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      tolerations:
        {{- if .Values.ollama.gpu.enabled }}
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
        {{- end }}
  volumeClaimTemplates:
  - metadata:
      name: ollama-volume
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: {{ .Values.ollama.volumeSize }}
